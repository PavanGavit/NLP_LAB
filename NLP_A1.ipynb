{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j_6nkrKypgVE",
        "outputId": "8259f0d2-c4dc-4145-ba89-55249c06458f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import (\n",
        "    WordPunctTokenizer,\n",
        "    TreebankWordTokenizer,\n",
        "    TweetTokenizer,\n",
        "    MWETokenizer\n",
        ")\n",
        "from nltk.stem import PorterStemmer, SnowballStemmer, WordNetLemmatizer\n",
        "\n",
        "# Sample texts for demonstration\n",
        "# Text 1: Standard sentence with contraction\n",
        "text_std = \"It's distinct from the other ones. Doesn't it look nice?\"\n",
        "# Text 2: Social media text with hashtag and handle\n",
        "text_tweet = \"Just loving the weather! #SunnyDay @WeatherChannel :)\"\n",
        "# Text 3: Text with Multi-Word Expression\n",
        "text_mwe = \"The United States of America is a large country.\"\n",
        "\n",
        "print(\"--- 1. TOKENIZATION ---\")\n",
        "\n",
        "# A. Whitespace Tokenization\n",
        "# (Simple split by space, often leaves punctuation attached to words)\n",
        "whitespace_tokens = text_std.split()\n",
        "print(f\"Whitespace:      {whitespace_tokens}\")\n",
        "\n",
        "# B. Punctuation-based Tokenization\n",
        "# (Splits on whitespace and punctuation distinctively)\n",
        "punct_tokenizer = WordPunctTokenizer()\n",
        "punct_tokens = punct_tokenizer.tokenize(text_std)\n",
        "print(f\"Punctuation:     {punct_tokens}\")\n",
        "\n",
        "# C. Treebank Tokenization (Standard NLTK method)\n",
        "# (Uses Penn Treebank conventions, e.g., separating contractions like \"n't\")\n",
        "treebank_tokenizer = TreebankWordTokenizer()\n",
        "treebank_tokens = treebank_tokenizer.tokenize(text_std)\n",
        "print(f\"Treebank:        {treebank_tokens}\")\n",
        "\n",
        "# D. Tweet Tokenization\n",
        "# (Preserves emojis, hashtags, and handles)\n",
        "tweet_tokenizer = TweetTokenizer()\n",
        "tweet_tokens = tweet_tokenizer.tokenize(text_tweet)\n",
        "print(f\"Tweet:           {tweet_tokens}\")\n",
        "\n",
        "# E. MWE (Multi-Word Expression) Tokenization\n",
        "# (Merges specific words into a single token)\n",
        "mwe_tokenizer = MWETokenizer([('United', 'States'), ('United', 'States', 'of', 'America')])\n",
        "# Note: MWE tokenizer usually runs on an already tokenized list\n",
        "base_tokens = text_mwe.split()\n",
        "mwe_tokens = mwe_tokenizer.tokenize(base_tokens)\n",
        "print(f\"MWE:             {mwe_tokens}\")\n",
        "\n",
        "\n",
        "print(\"\\n--- 2. STEMMING ---\")\n",
        "# Reducing words to their root/base form (often by chopping off ends).\n",
        "\n",
        "words_to_stem = [\"running\", \"generously\", \"happily\", \"organization\", \"wolves\"]\n",
        "porter = PorterStemmer()\n",
        "snowball = SnowballStemmer(language='english')\n",
        "\n",
        "print(f\"{'Original':<15} | {'Porter':<15} | {'Snowball':<15}\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "for word in words_to_stem:\n",
        "    p_stem = porter.stem(word)\n",
        "    s_stem = snowball.stem(word)\n",
        "    print(f\"{word:<15} | {p_stem:<15} | {s_stem:<15}\")\n",
        "\n",
        "\n",
        "print(\"\\n--- 3. LEMMATIZATION ---\")\n",
        "# Reducing words to their meaningful base form (lemma) using vocabulary analysis.\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "words_to_lemmatize = [\"better\", \"running\", \"wolves\", \"are\", \"corpora\"]\n",
        "\n",
        "print(f\"{'Original':<15} | {'Lemma (Noun)':<15} | {'Lemma (Verb)':<15}\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "for word in words_to_lemmatize:\n",
        "    # Default is noun (n)\n",
        "    lemma_noun = lemmatizer.lemmatize(word)\n",
        "    # Specifying 'v' handles the word as a verb\n",
        "    lemma_verb = lemmatizer.lemmatize(word, pos='v')\n",
        "\n",
        "    print(f\"{word:<15} | {lemma_noun:<15} | {lemma_verb:<15}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RejpcsLSqA4P",
        "outputId": "a4881b0b-df1e-4e54-a8ab-2654013a351d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- 1. TOKENIZATION ---\n",
            "Whitespace:      [\"It's\", 'distinct', 'from', 'the', 'other', 'ones.', \"Doesn't\", 'it', 'look', 'nice?']\n",
            "Punctuation:     ['It', \"'\", 's', 'distinct', 'from', 'the', 'other', 'ones', '.', 'Doesn', \"'\", 't', 'it', 'look', 'nice', '?']\n",
            "Treebank:        ['It', \"'s\", 'distinct', 'from', 'the', 'other', 'ones.', 'Does', \"n't\", 'it', 'look', 'nice', '?']\n",
            "Tweet:           ['Just', 'loving', 'the', 'weather', '!', '#SunnyDay', '@WeatherChannel', ':)']\n",
            "MWE:             ['The', 'United_States_of_America', 'is', 'a', 'large', 'country.']\n",
            "\n",
            "--- 2. STEMMING ---\n",
            "Original        | Porter          | Snowball       \n",
            "--------------------------------------------------\n",
            "running         | run             | run            \n",
            "generously      | gener           | generous       \n",
            "happily         | happili         | happili        \n",
            "organization    | organ           | organ          \n",
            "wolves          | wolv            | wolv           \n",
            "\n",
            "--- 3. LEMMATIZATION ---\n",
            "Original        | Lemma (Noun)    | Lemma (Verb)   \n",
            "--------------------------------------------------\n",
            "better          | better          | better         \n",
            "running         | running         | run            \n",
            "wolves          | wolf            | wolves         \n",
            "are             | are             | be             \n",
            "corpora         | corpus          | corpora        \n"
          ]
        }
      ]
    }
  ]
}