{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOYGbZuHCFSx+J0DUb4w2Xd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PavanGavit/NLP_LAB/blob/main/NLP_A2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KCbLgom-qr29",
        "outputId": "906ba794-9337-40c8-9249-888f6f11c01f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Collecting gensim\n",
            "  Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (8.4 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: smart_open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.5.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2025.11.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart_open>=1.8.1->gensim) (2.0.1)\n",
            "Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (27.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.9/27.9 MB\u001b[0m \u001b[31m29.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: gensim\n",
            "Successfully installed gensim-4.4.0\n"
          ]
        }
      ],
      "source": [
        "pip install scikit-learn gensim pandas nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# --- THE FIX ---\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt_tab')\n",
        "except LookupError:\n",
        "    nltk.download('punkt_tab')\n",
        "    nltk.download('punkt')\n",
        "# ---------------\n",
        "\n",
        "# Sample Corpus\n",
        "corpus = [\n",
        "    \"Data science is amazing and fun\",\n",
        "    \"Science involves data and experiments\",\n",
        "    \"Fun experiments lead to amazing data\"\n",
        "]\n",
        "\n",
        "# This line should now work without error\n",
        "tokenized_corpus = [word_tokenize(doc.lower()) for doc in corpus]\n",
        "\n",
        "print(\"Corpus Loaded and Tokenized Successfully!\")\n",
        "print(tokenized_corpus[0])\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IODVYbG9rZ3l",
        "outputId": "b5495287-c116-4ef9-ff30-78ba53b6078e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Corpus Loaded and Tokenized Successfully!\n",
            "['data', 'science', 'is', 'amazing', 'and', 'fun']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "# --- 1. SETUP & FIXES ---\n",
        "# Download necessary NLTK data (fixing the LookupError)\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt_tab')\n",
        "except LookupError:\n",
        "    nltk.download('punkt_tab')\n",
        "    nltk.download('punkt')\n",
        "\n",
        "# Sample Corpus\n",
        "corpus = [\n",
        "    \"Data science is amazing and fun\",\n",
        "    \"Science involves data and experiments\",\n",
        "    \"Fun experiments lead to amazing data\"\n",
        "]\n",
        "\n",
        "# Preprocessing: Tokenize for Word2Vec\n",
        "tokenized_corpus = [word_tokenize(doc.lower()) for doc in corpus]\n",
        "print(\"--- Data Loaded ---\")\n",
        "\n",
        "# --- 2. BAG OF WORDS (COUNT) ---\n",
        "# We must define 'bow_matrix' here so it exists for the next steps\n",
        "count_vectorizer = CountVectorizer()\n",
        "bow_matrix = count_vectorizer.fit_transform(corpus)\n",
        "\n",
        "bow_df = pd.DataFrame(\n",
        "    bow_matrix.toarray(),\n",
        "    columns=count_vectorizer.get_feature_names_out()\n",
        ")\n",
        "print(\"\\n--- Bag-of-Words (Count) ---\")\n",
        "print(bow_df)\n",
        "\n",
        "# --- 3. BAG OF WORDS (NORMALIZED) ---\n",
        "# Now 'bow_matrix' is defined, so this will work\n",
        "tf_transformer = TfidfTransformer(use_idf=False, norm='l1')\n",
        "normalized_matrix = tf_transformer.fit_transform(bow_matrix)\n",
        "\n",
        "normalized_df = pd.DataFrame(\n",
        "    normalized_matrix.toarray(),\n",
        "    columns=count_vectorizer.get_feature_names_out()\n",
        ")\n",
        "print(\"\\n--- Normalized Counts (TF) ---\")\n",
        "print(normalized_df.round(2))\n",
        "\n",
        "# --- 4. TF-IDF ---\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(corpus)\n",
        "\n",
        "tfidf_df = pd.DataFrame(\n",
        "    tfidf_matrix.toarray(),\n",
        "    columns=tfidf_vectorizer.get_feature_names_out()\n",
        ")\n",
        "print(\"\\n--- TF-IDF ---\")\n",
        "print(tfidf_df.round(2))\n",
        "\n",
        "# --- 5. WORD2VEC EMBEDDINGS ---\n",
        "# Training the model\n",
        "model = Word2Vec(sentences=tokenized_corpus, vector_size=10, window=2, min_count=1, workers=4)\n",
        "\n",
        "print(\"\\n--- Word2Vec Embedding for 'science' ---\")\n",
        "print(model.wv['science'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p-L3Uxnpr3r4",
        "outputId": "f81348d1-f146-4f2e-f895-3230ade2ea43"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Data Loaded ---\n",
            "\n",
            "--- Bag-of-Words (Count) ---\n",
            "   amazing  and  data  experiments  fun  involves  is  lead  science  to\n",
            "0        1    1     1            0    1         0   1     0        1   0\n",
            "1        0    1     1            1    0         1   0     0        1   0\n",
            "2        1    0     1            1    1         0   0     1        0   1\n",
            "\n",
            "--- Normalized Counts (TF) ---\n",
            "   amazing   and  data  experiments   fun  involves    is  lead  science    to\n",
            "0     0.17  0.17  0.17         0.00  0.17       0.0  0.17  0.00     0.17  0.00\n",
            "1     0.00  0.20  0.20         0.20  0.00       0.2  0.00  0.00     0.20  0.00\n",
            "2     0.17  0.00  0.17         0.17  0.17       0.0  0.00  0.17     0.00  0.17\n",
            "\n",
            "--- TF-IDF ---\n",
            "   amazing   and  data  experiments   fun  involves    is  lead  science    to\n",
            "0     0.40  0.40  0.31         0.00  0.40      0.00  0.52  0.00     0.40  0.00\n",
            "1     0.00  0.43  0.34         0.43  0.00      0.57  0.00  0.00     0.43  0.00\n",
            "2     0.38  0.00  0.29         0.38  0.38      0.00  0.00  0.49     0.00  0.49\n",
            "\n",
            "--- Word2Vec Embedding for 'science' ---\n",
            "[-0.08157917  0.04495798 -0.04137076  0.00824536  0.08498619 -0.04462177\n",
            "  0.045175   -0.0678696  -0.03548489  0.09398508]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Qkoa_fxZr4H8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}