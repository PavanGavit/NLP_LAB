{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPVFWZK/tjRTr4wLssr8Fo8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PavanGavit/NLP_LAB/blob/main/NLP_A3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zYb5gX0ps1uC",
        "outputId": "2355542b-45b5-4004-b878-8021446b29ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Original Data ---\n",
            "                                                text category\n",
            "0     The quick brown fox jumps over the lazy dog!!!  animals\n",
            "1           Data Science is... simply amazing & fun.     tech\n",
            "2  I love machine learning and artificial intelli...     tech\n",
            "3     Python is a great programming language for AI.     tech\n",
            "4              The dog is barking loudly at the fox.  animals\n",
            "\n",
            "--- After Cleaning & Lemmatization ---\n",
            "                                                text  \\\n",
            "0     The quick brown fox jumps over the lazy dog!!!   \n",
            "1           Data Science is... simply amazing & fun.   \n",
            "2  I love machine learning and artificial intelli...   \n",
            "3     Python is a great programming language for AI.   \n",
            "4              The dog is barking loudly at the fox.   \n",
            "\n",
            "                                    cleaned_text  \n",
            "0                  quick brown fox jump lazy dog  \n",
            "1                data science simply amazing fun  \n",
            "2  love machine learning artificial intelligence  \n",
            "3           python great programming language ai  \n",
            "4                         dog barking loudly fox  \n",
            "\n",
            "--- After Label Encoding ---\n",
            "  category  category_encoded\n",
            "0  animals                 0\n",
            "1     tech                 1\n",
            "2     tech                 1\n",
            "3     tech                 1\n",
            "4  animals                 0\n",
            "\n",
            "--- TF-IDF Features (First 5 columns) ---\n",
            "         ai   amazing  artificial   barking     brown\n",
            "0  0.000000  0.000000    0.000000  0.000000  0.434297\n",
            "1  0.000000  0.447214    0.000000  0.000000  0.000000\n",
            "2  0.000000  0.000000    0.447214  0.000000  0.000000\n",
            "3  0.447214  0.000000    0.000000  0.000000  0.000000\n",
            "4  0.000000  0.000000    0.000000  0.550329  0.000000\n",
            "\n",
            "[SUCCESS] Dataset saved to 'processed_dataset.csv'\n",
            "[SUCCESS] TF-IDF Model saved to 'tfidf_model.pkl'\n",
            "[SUCCESS] Label Encoder saved to 'label_encoder.pkl'\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import pickle\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# --- 1. SETUP & DOWNLOADS ---\n",
        "# Ensure all necessary NLTK data is available\n",
        "resources = ['punkt_tab', 'wordnet', 'stopwords', 'omw-1.4']\n",
        "for resource in resources:\n",
        "    try:\n",
        "        nltk.data.find(f'tokenizers/{resource}' if 'punkt' in resource else f'corpora/{resource}')\n",
        "    except LookupError:\n",
        "        nltk.download(resource, quiet=True)\n",
        "\n",
        "# --- 2. CREATE SAMPLE DATASET ---\n",
        "data = {\n",
        "    'text': [\n",
        "        \"The quick brown fox jumps over the lazy dog!!!\",\n",
        "        \"Data Science is... simply amazing & fun.\",\n",
        "        \"I love machine learning and artificial intelligence.\",\n",
        "        \"Python is a great programming language for AI.\",\n",
        "        \"The dog is barking loudly at the fox.\"\n",
        "    ],\n",
        "    'category': ['animals', 'tech', 'tech', 'tech', 'animals']\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "print(\"--- Original Data ---\")\n",
        "print(df)\n",
        "\n",
        "# --- 3. DEFINE CLEANING PIPELINE ---\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # A. Lowercasing\n",
        "    text = text.lower()\n",
        "\n",
        "    # B. Text Cleaning (Remove special characters/punctuation)\n",
        "    # This regex keeps only alphanumeric chars and spaces\n",
        "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
        "\n",
        "    # C. Tokenization\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # D. Stop Word Removal & Lemmatization\n",
        "    cleaned_tokens = [\n",
        "        lemmatizer.lemmatize(token)  # Lemmatize\n",
        "        for token in tokens\n",
        "        if token not in stop_words   # Remove Stop Words\n",
        "    ]\n",
        "\n",
        "    # Join back into a string\n",
        "    return \" \".join(cleaned_tokens)\n",
        "\n",
        "# Apply the preprocessing\n",
        "df['cleaned_text'] = df['text'].apply(preprocess_text)\n",
        "\n",
        "print(\"\\n--- After Cleaning & Lemmatization ---\")\n",
        "print(df[['text', 'cleaned_text']])\n",
        "\n",
        "# --- 4. LABEL ENCODING ---\n",
        "# Converts text labels ('tech', 'animals') into numbers (1, 0)\n",
        "label_encoder = LabelEncoder()\n",
        "df['category_encoded'] = label_encoder.fit_transform(df['category'])\n",
        "\n",
        "print(\"\\n--- After Label Encoding ---\")\n",
        "print(df[['category', 'category_encoded']])\n",
        "\n",
        "# --- 5. TF-IDF VECTORIZATION ---\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(df['cleaned_text'])\n",
        "\n",
        "# Create a DataFrame for the TF-IDF output (for visualization)\n",
        "tfidf_df = pd.DataFrame(\n",
        "    tfidf_matrix.toarray(),\n",
        "    columns=tfidf_vectorizer.get_feature_names_out()\n",
        ")\n",
        "\n",
        "print(\"\\n--- TF-IDF Features (First 5 columns) ---\")\n",
        "print(tfidf_df.iloc[:, :5]) # Printing only first 5 columns to save space\n",
        "\n",
        "# --- 6. SAVE OUTPUTS ---\n",
        "\n",
        "# A. Save the processed dataset to CSV\n",
        "df.to_csv(\"processed_dataset.csv\", index=False)\n",
        "print(\"\\n[SUCCESS] Dataset saved to 'processed_dataset.csv'\")\n",
        "\n",
        "# B. Save the artifacts (Models/Encoders) using Pickle\n",
        "# This allows you to load the exact same vectorizer logic later\n",
        "with open(\"tfidf_model.pkl\", \"wb\") as f:\n",
        "    pickle.dump(tfidf_vectorizer, f)\n",
        "\n",
        "with open(\"label_encoder.pkl\", \"wb\") as f:\n",
        "    pickle.dump(label_encoder, f)\n",
        "\n",
        "print(\"[SUCCESS] TF-IDF Model saved to 'tfidf_model.pkl'\")\n",
        "print(\"[SUCCESS] Label Encoder saved to 'label_encoder.pkl'\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9PvcqMJYttbM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}